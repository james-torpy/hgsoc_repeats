{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is the raw_dir:\n",
      "/Users/jamestorpy/clusterHome//projects/hgsoc_repeats/RNA-seq//raw_files/fullsamples/bowtell_primary/\n"
     ]
    }
   ],
   "source": [
    "#!/home/jamtor/local/bin/Python-3.6.3/bin/python3\n",
    "\n",
    "### 1.bamtofastq_autosubmit.py ###\n",
    "\n",
    "# This script detects which output files still must be created from the repeatsPipeline\n",
    "# and submits jobs for them #\n",
    "\n",
    "# run once and then input the following command into crontab for automation:\n",
    "# */10 * * * * source /etc/environment; source /usr/bin; source /home/jamtor/.bashrc; \\\n",
    "# /home/jamtor/local/bin/Python-3.6.3/bin/python3.6 \\\n",
    "# /share/ClusterShare/thingamajigs/jamtor/projects/hgsoc_repeats/RNA-seq/scripts/repeatsPipeline/exp9/job_autosubmit.py\n",
    "\n",
    "### 0. Define starting variables ###\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import re\n",
    "import time\n",
    "\n",
    "os.system('source /home/jamtor/.bashrc')\n",
    "os.system('module load gi/zlib/1.2.8')\n",
    "os.system('module load phuluu/samtools/1.4')\n",
    "\n",
    "# make directory hierachy:\n",
    "projectname = 'hgsoc_repeats'\n",
    "sample_type = 'fullsamples/bowtell_primary'\n",
    "exp_name = 'rsem_no_mmappers'\n",
    "total_no_jobs = 4\n",
    "q = 'short'\n",
    "upper_core_lim = 200\n",
    "lower_core_lim = 200\n",
    "\n",
    "# home_dir is where input/intermediate files are located:\n",
    "#home_dir = '/share/ScratchGeneral/jamtor/'\n",
    "home_dir = '/Users/jamestorpy/clusterHome/'\n",
    "# home_dir2 is where scripts are located and final outputs will be stored:\n",
    "#home_dir2 = '/share/ClusterShare/thingamajigs/jamtor'\n",
    "home_dir2 = '/Users/jamestorpy/clusterHome2/'\n",
    "\n",
    "project_dir = home_dir + '/projects/' + projectname + '/RNA-seq/'\n",
    "raw_dir = project_dir + '/raw_files/' + sample_type + '/'\n",
    "results_dir = project_dir + '/results/'\n",
    "\n",
    "print('')\n",
    "print('This is the raw_dir:')\n",
    "print(raw_dir)\n",
    "\n",
    "# scripts/logs directories:\n",
    "script_dir = home_dir2 + '/projects/hgsoc_repeats/RNA-seq/scripts/' + exp_name + '/'\n",
    "log_dir = project_dir + '/logs/' + exp_name + '/'\n",
    "old_log_dir = project_dir + '/logs/' + exp_name + '/old/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(old_log_dir, exist_ok=True)\n",
    "\n",
    "print('')\n",
    "print('The log_dir is:')\n",
    "print(log_dir)\n",
    "print('')\n",
    "print('The script_dir is:')\n",
    "print(script_dir)\n",
    "\n",
    "# add indicator that crontab is running:\n",
    "ind = open(script_dir + '/indicator.txt', 'w')\n",
    "ind.write(str(time.strftime('%H:%M:%S')))\n",
    "\n",
    "# define other output directories:\n",
    "fastq_dir=raw_dir + '/fastq/'\n",
    "os.makedirs(fastq_dir, exist_ok=True)\n",
    "\n",
    "ribo_dir = results_dir + '/star/ribo/' + exp_name + '/'\n",
    "os.makedirs(ribo_dir, exist_ok=True)\n",
    "\n",
    "gc_dir = results_dir + '/star/GC/' + exp_name + '/'\n",
    "os.makedirs(gc_dir, exist_ok=True)\n",
    "\n",
    "rsem_dir = results_dir + '/rsem/' + exp_name + '/'\n",
    "os.makedirs(rsem_dir, exist_ok=True)\n",
    "\n",
    "print('')\n",
    "print('The fastq_dir is:')\n",
    "print(fastq_dir)\n",
    "print('')\n",
    "print('The ribo_dir is:')\n",
    "print(ribo_dir)\n",
    "print('')\n",
    "print('The gc_dir is:')\n",
    "print(gc_dir)\n",
    "print('')\n",
    "print('The rsem_dir is:')\n",
    "print(rsem_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'script_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d1f42fb597ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# save entire qstat output for 1st core number checkpoint:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"qstat -u '*' | grep jamtor > \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mscript_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/qstat.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# save qstat job output as file:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'script_dir' is not defined"
     ]
    }
   ],
   "source": [
    "### 1. Set up core number checkpoints ###\n",
    "\n",
    "# save entire qstat output for 1st core number checkpoint:\n",
    "os.system(\"qstat -u '*' | grep jamtor > \" + script_dir + \"/qstat.txt\")\n",
    "\n",
    "# save qstat job output as file:\n",
    "os.system(\"qstat -u '*' | grep jamtor | grep -v QRLOGIN | awk '{print $3}' > \" + script_dir + \"/qstat_jobs.txt\")\n",
    "\n",
    "# fetch job ids currently running or queued:\n",
    "qstat_jobs = []\n",
    "with open(script_dir + '/qstat_jobs.txt') as f:\n",
    "    for line in f:\n",
    "        qstat_jobs.append(\n",
    "            re.sub(\n",
    "                '\\n', '', line\n",
    "            )\n",
    "        )\n",
    "print('')\n",
    "print('Jobs already running or queued:')\n",
    "print(qstat_jobs)\n",
    "\n",
    "# save qstat cores output as file:\n",
    "os.system(\"qstat -u '*' | grep jamtor | grep -e 'q@' | grep -v QRLOGIN | awk '{print $9}' > \" + script_dir + \"/qstat_cores.txt\")\n",
    "\n",
    "# fetch core number:\n",
    "slots = 0\n",
    "with open(script_dir + '/qstat_cores.txt') as f:\n",
    "    for line in f:\n",
    "        slots = slots + int(re.sub('\\\\n', '', line))\n",
    "\n",
    "# if too many cores are being used, remove all but running jobs:\n",
    "#if slots > upper_core_lim:\n",
    "#    print('Too many cores being used, removing all except first 8 jobs')\n",
    "#    qstat = pd.read_csv(script_dir + '/qstat.txt', header = None)\n",
    "#    for line in qstat[8:].iterrows():\n",
    "#        j_id = str.split(str(line[1]))[1]\n",
    "#        print(j_id)\n",
    "#        os.system('qdel ' + j_id)\n",
    "\n",
    "# create list of jobs currently running or queued in qstat:\n",
    "running_jobs = []\n",
    "if not os.stat(script_dir + '/qstat.txt').st_size == 0:\n",
    "    index = 0\n",
    "    qstat = pd.read_csv(script_dir + '/qstat.txt', header = None)\n",
    "    for line in qstat.iterrows():\n",
    "        if index == 0:\n",
    "            running_jobs = [str.split(str(line[1]))[1]]\n",
    "        else:\n",
    "            running_jobs[index] = str.split(str(line[1]))[1]\n",
    "\n",
    "\n",
    "if slots < lower_core_lim:\n",
    "    print('')\n",
    "    print('Less than ' + str(lower_core_lim) + ' cores being used! Submitting more jobs...')\n",
    "\n",
    "    ### 2. Check for any errors in the log files and delete output files\n",
    "    # from erroneous jobs: ###\n",
    "\n",
    "    sterms = ['error', 'ERROR', 'Error', 'halted', 'unexpected']\n",
    "    redo = []\n",
    "    for log in os.listdir(log_dir):\n",
    "        if '.o' in log:\n",
    "            for term in sterms:\n",
    "                if os.path.isfile(log_dir + '/' + log):\n",
    "                    if term in open(log_dir + '/' + log).read():\n",
    "                        redo.append(\n",
    "                            re.sub(\n",
    "                                '.o[0-9].*', '', os.path.basename(log)\t\n",
    "                            )\n",
    "                        )\n",
    "                        os.system('mv ' + log_dir + '/' + log + ' ' + old_log_dir)\n",
    "    redo = list(set(redo))\n",
    "    print('')\n",
    "    print('The following jobs will be redone:')\n",
    "    for r in redo:\n",
    "        print(r)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    ### 3. Check whether job has been previously completed ###\n",
    "\n",
    "    if os.path.isfile(script_dir + '/done_samples.txt'):\n",
    "        with open(script_dir + '/done_samples.txt') as f:\n",
    "            done_ids = f.readlines()\n",
    "        # remove whitespace characters like `\\n` at the end of each line\n",
    "        done_ids = [x.strip() for x in done_ids]\n",
    "        print('')\n",
    "        print(done_ids)\n",
    "    else:\n",
    "        done_ids = []\n",
    "\n",
    "\n",
    "    ### 4. Define specific job parameters for each sample ###\n",
    "    \n",
    "    # read in raw files and fetch ids:\n",
    "    in_files = glob.glob(raw_dir + '*.bam')\n",
    "    print(in_files)\n",
    "    #in_files.extend(glob.glob(raw_dir + 'FT[1-7].bam'))\n",
    "\t#in_files.extend(glob.glob(raw_dir + 'prPT1[2-7].bam'))\n",
    "\t#in_files.extend(glob.glob(raw_dir + 'rfPT[1-9].bam'))\n",
    "\t#in_files.extend(glob.glob(raw_dir + 'rcAF[1-9].bam'))\n",
    "\t#in_files.extend(glob.glob(raw_dir + 'arPT[1-9].bam'))\n",
    "\t#in_files.extend(glob.glob(raw_dir + 'msST[1-5].bam'))\n",
    "\t#in_files.extend(glob.glob(raw_dir + 'mrPT[1-8].bam'))\n",
    "\t#in_files.extend(glob.glob(raw_dir + 'erPT[1-8].bam'))\n",
    "    #in_files = [raw_dir + '/prPT9.bam', raw_dir + '/prPT10.bam', raw_dir \\\n",
    "    #+ '/prPT11.bam', raw_dir + '/rcAF16.bam']\n",
    "    #in_files = [raw_dir + '/FT3.bam']\n",
    "    \n",
    "for infile in in_files:\n",
    "    if slots < lower_core_lim:\n",
    "        print('')\n",
    "        print('')\n",
    "        print('The infile is ' + infile)\n",
    "        u_id = re.sub(\n",
    "            '.bam', '', os.path.basename(infile)\n",
    "        )\n",
    "        print('')\n",
    "        print('The u_id is: ' + u_id)\n",
    "\n",
    "        if u_id not in done_ids:\n",
    "    \n",
    "\n",
    "            ### Job 0 inputs ###\n",
    "            # bam to fastq conversion #\n",
    "            \n",
    "            cores = ['2']\n",
    "        \n",
    "            inputs1 = [raw_dir + '/' + u_id + '.bam']\n",
    "            inputs2 = ['none']\n",
    "            inputs3 = ['none']\n",
    "            inputs4 = ['none']\n",
    "        \n",
    "            outputs1 = [fastq_dir + '/' + u_id + '.1.fastq.gz']\n",
    "            outputs2 = [fastq_dir + '/' + u_id + '.2.fastq.gz']\n",
    "            outputs3 = [fastq_dir + '/' + u_id + '.unpaired.1.fastq.gz']\n",
    "            outputs4 = [fastq_dir + '/' + u_id + '.unpaired.2.fastq.gz']\n",
    "    \n",
    "            extra_params = ['none']\n",
    "            \n",
    "            scripts = [script_dir + '/0.bamtofastq.bash']\n",
    "            \n",
    "            qsub_params = ['none']\n",
    "    \n",
    "            dependent_job = ['none']\n",
    "    \n",
    "            # define the minimum size the output files should be before\n",
    "            # the job is accepted as complete\n",
    "            min_output_size1 = [3500000000]\n",
    "            min_output_size2 = [3500000000]\n",
    "            min_output_size3 = ['none']\n",
    "            min_output_size4 = ['none']\n",
    "            \n",
    "            \n",
    "            ### Job 1 inputs ###\n",
    "            # alignment of fastqs to gc transcriptome reference #\n",
    "            \n",
    "            cores.append('6')\n",
    "            \n",
    "            inputs1.append(fastq_dir + '/' + u_id + '.1.fastq.gz')\n",
    "            inputs2.append(fastq_dir + '/' + u_id + '.2.fastq.gz')\n",
    "            inputs3.append('none')\n",
    "            inputs4.append('none')\n",
    "            \n",
    "            outputs1.append(gc_dir + '/' + u_id + '/Log.final.out')\n",
    "            outputs2.append(gc_dir + '/' + u_id + '/Aligned.toTranscriptome.out.bam')\n",
    "            outputs3.append(gc_dir + '/' + u_id + '/SJ.out.tab')\n",
    "            outputs4.append('none')\n",
    "    \n",
    "            extra_params.append('none')\n",
    "    \n",
    "            scripts.append(script_dir + '/1.star_ercc.bash')\n",
    "            \n",
    "            qsub_params.append('none')\n",
    "    \n",
    "            dependent_job.append('j0')\n",
    "    \n",
    "            # define the minimum size the output files should be before\n",
    "            # the job is accepted as complete\n",
    "            min_output_size1.append('none')\n",
    "            min_output_size2.append('none')\n",
    "            min_output_size3.append('none')\n",
    "            min_output_size4.append('none')\n",
    "    \n",
    "    \n",
    "            ### Job 2 inputs ###\n",
    "            # sort bams by name #\n",
    "            \n",
    "            cores.append('6')\n",
    "            \n",
    "            inputs1.append(gc_dir + '/' + u_id + '/Aligned.toTranscriptome.out.bam')\n",
    "            inputs2.append('none')\n",
    "            inputs3.append('none')\n",
    "            inputs4.append('none')\n",
    "            \n",
    "            outputs1.append(gc_dir + '/' + u_id + '/Aligned.toTranscriptome.novosortedByName.out.bam')\n",
    "            outputs2.append('none')\n",
    "            outputs3.append('none')\n",
    "            outputs4.append('none')\n",
    "    \n",
    "            extra_params.append('22G')\n",
    "    \n",
    "            scripts.append(script_dir + '/2.novosortName.bash')\n",
    "            \n",
    "            qsub_params.append('none')\n",
    "    \n",
    "            dependent_job.append('j1')\n",
    "    \n",
    "            # define the minimum size the output files should be before\n",
    "            # the job is accepted as complete\n",
    "            min_output_size1.append(12000000000)\n",
    "            min_output_size2.append('none')\n",
    "            min_output_size3.append('none')\n",
    "            min_output_size4.append('none')\n",
    "    \n",
    "    \n",
    "            ### Job 3 inputs ###\n",
    "            # count gc transcripts from bam using rsem #\n",
    "            \n",
    "            cores.append('6')\n",
    "            \n",
    "            inputs1.append(gc_dir + '/' + u_id + '/Aligned.toTranscriptome.novosortedByName.out.bam')\n",
    "            inputs2.append('none')\n",
    "            inputs3.append('none')\n",
    "            inputs4.append('none')\n",
    "            \n",
    "            outputs1.append(rsem_dir + '/' + u_id + '/' + u_id + '.transcriptome.genes.results')\n",
    "            outputs2.append('none')\n",
    "            outputs3.append('none')\n",
    "            outputs4.append('none')\n",
    "    \n",
    "            extra_params.append('none')\n",
    "    \n",
    "            scripts.append(script_dir + '/3.rsem_ercc.bash')\n",
    "            \n",
    "            qsub_params.append('none')\n",
    "    \n",
    "            dependent_job.append('j2')\n",
    "    \n",
    "            # define the minimum size the output files should be before\n",
    "            # the job is accepted as complete\n",
    "            min_output_size1.append(2000)\n",
    "            min_output_size2.append('none')\n",
    "            min_output_size3.append('none')\n",
    "            min_output_size4.append('none')\n",
    "           \n",
    "            for i in range(total_no_jobs):\n",
    "                print('')\n",
    "                print('')\n",
    "                print('For job number ' + str(i) + ':')\n",
    "    \n",
    "                # set up job holding if necessary:\n",
    "                if dependent_job[i] == 'none':\n",
    "                    holdj = ' '\n",
    "                else:\n",
    "                    holdj = ' -hold_jid ' + dependent_job[i] + '_' + u_id\n",
    "    \n",
    "                print('')\n",
    "                print('This is the hold string:')\n",
    "                print(holdj)\n",
    "                    \n",
    "                # define extra_params:\n",
    "                if extra_params:\n",
    "                    if extra_params[i] == 'none':\n",
    "                        extra_params[i] = ''\n",
    "    \n",
    "                # define qsub_params:\n",
    "                if qsub_params:\n",
    "                    if qsub_params[i] == 'none':\n",
    "                        qsub_params[i] = ''\n",
    "    \n",
    "                # define dependent_job:\n",
    "                if dependent_job:\n",
    "                    if dependent_job[i] == 'none':\n",
    "                        dependent_job[i] = ''\n",
    "    \n",
    "                \n",
    "                inputs = [inputs1[i]]\n",
    "                for input in [inputs2[i], inputs3[i], inputs4[i]]:\n",
    "                    if input !='none':\n",
    "                        inputs.append(input)\n",
    "                \n",
    "                outputs = [outputs1[i]]\n",
    "                for output in [outputs2[i], outputs3[i], outputs4[i]]:\n",
    "                    if output !='none':\n",
    "                        outputs.append(output)\n",
    "    \n",
    "                min_output_sizes = [min_output_size1[i]]\n",
    "                for size in [min_output_size2[i], min_output_size3[i], min_output_size4[i]]:\n",
    "                        min_output_sizes.append(size)\n",
    "    \n",
    "                print('')\n",
    "                print('min_output_sizes are:')\n",
    "                print(min_output_sizes)\n",
    "    \n",
    "                # determine whether output files are min file size required:\n",
    "                min_size_pass = []\n",
    "                for j, out in enumerate(outputs):\n",
    "                    if os.path.isfile(out):\n",
    "                        if min_output_sizes[j] != 'none':\n",
    "                            min_size_pass.append(os.path.getsize(out) > min_output_sizes[j])\n",
    "    \n",
    "                # if all outputs are minimum size, record size_pass as True, if not, False:\n",
    "                size_pass = all(min_size_pass)\n",
    "                print('')\n",
    "                print('sizepass is ' + str(size_pass))\n",
    "    \n",
    "    \n",
    "                # define qsub command for jobs:\n",
    "                q_command = '/opt/gridengine/bin/linux-x64/qsub ' \\\n",
    "                          + '-q ' + q + '.q ' \\\n",
    "                          + '-N j' + str(i) + '_' + u_id \\\n",
    "                          + holdj \\\n",
    "                          + ' -b y -wd ' \\\n",
    "                          + log_dir \\\n",
    "                          + ' -j y -R y ' \\\n",
    "                          + '-pe smp ' + cores[i] + ' ' \\\n",
    "                          + qsub_params[i] \\\n",
    "                          + ' -V ' + scripts[i] + ' ' \\\n",
    "                          + cores[i] + ' ' \\\n",
    "                          + ' '.join(inputs) + ' ' \\\n",
    "                          + ' '.join(outputs) + ' ' \\\n",
    "                          + extra_params[i]\n",
    "            \n",
    "    \n",
    "                ### 5. Run jobs if they meet the conditions required ###\n",
    "    \n",
    "                print('')\n",
    "                if not all([os.path.isfile(inp) for inp in inputs]):\n",
    "                    print('At least one job ' + str(i) + ' input file does not exist, holding job')\n",
    "    \n",
    "                elif all([os.path.isfile(inp) for inp in inputs]) and not all([os.path.isfile(outp) \\\n",
    "                \tfor outp in outputs]) and not 'j' + str(i) + '_' + u_id in qstat_jobs:\n",
    "                    print('Submitting job ' + str(i) + ' with input/s or in dir/s:')\n",
    "                    print(inputs)\n",
    "                    print('')\n",
    "                    print('and output/s or out dir/s:')\n",
    "                    print(outputs)\n",
    "    \n",
    "                    os.system(q_command)\n",
    "    \n",
    "                elif 'j' + str(i) + '_' + u_id in redo and not 'j' + str(i) + '_' + u_id in qstat_jobs:\n",
    "                    cores[i] = int(cores[i]) + 2\n",
    "                    print('Job previously encountered an error!')\n",
    "                    print('Submitting job ' + str(i) + ' with input/s or in dir/s and ' + str(cores[i]) + ' cores:')\n",
    "                    print(inputs)\n",
    "                    print('')\n",
    "                    print('and output/s or out dir/s:')\n",
    "                    print(outputs)\n",
    "    \n",
    "                    os.system(q_command)\n",
    "    \n",
    "                # run job if outputs exist but are not the minimum file size:\n",
    "                elif all([os.path.isfile(outp) for outp in outputs]) and size_pass == False and not \\\n",
    "                    'j' + str(i) + '_' + u_id in qstat_jobs:\n",
    "                    print('Job outputs are not the minimum size required!')\n",
    "                    print('Submitting job ' + str(i) + ' with input/s or in dir/s and ' + str(cores[i]) + ' cores:')\n",
    "                    print(inputs)\n",
    "                    print('')\n",
    "                    print('and output/s or out dir/s:')\n",
    "                    print(outputs)\n",
    "    \n",
    "                    os.system(q_command)\n",
    "    \n",
    "                elif all([os.path.isfile(outp) for outp in outputs]) and size_pass == True:\n",
    "                    print('Job ' + str(i) + ' has previously been completed, no need to run')\n",
    "                \n",
    "                elif 'j' + str(i) + '_' + u_id in qstat_jobs:\n",
    "                    os.system('echo Job ' + str(i) + ' is currently running or queued, no need to run again')\n",
    "    \n",
    "    \n",
    "            ### 6. Remove files no longer needed ###\n",
    "            \n",
    "#                rm_cores = '1'\n",
    "#                \n",
    "#                rm1 = raw_dir + '/' + u_id + '.bam'\n",
    "#                rm2 = gc_dir + '/' + u_id + '/Aligned.out.bam'\n",
    "#        \n",
    "#                rm_script = (script_dir + '/7.remove_files.bash')\n",
    "#        \n",
    "#                rm_dependents = ['j0', 'j1', 'j2', 'j3']\n",
    "#        \n",
    "#        \n",
    "#                rm_files = [rm1, rm2]\n",
    "#        \n",
    "#                holdj = '-hold_jid '\n",
    "#                for d in rm_dependents:\n",
    "#                    holdj = holdj + d + '_' + u_id + ','\n",
    "#                \n",
    "#                # double check all outputs have been created:\n",
    "#                final_check = []\n",
    "#                for n, op in enumerate(outputs):\n",
    "#                    final_check.append(os.path.isfile(op))\n",
    "#    \n",
    "#                final_size_check = []\n",
    "#                for n, op in enumerate(outputs):\n",
    "#                    if os.path.isfile(op):\n",
    "#                        final_size_check.append(os.path.getsize(op) > 2000)\n",
    "#    \n",
    "#                if all(final_check):\n",
    "#                    for k, rm in enumerate(rm_files):\n",
    "#                        if not 'r' + str(k) + '_' + u_id in qstat_jobs:\n",
    "#                            print('')\n",
    "#                            print('Removing ' + rm)\n",
    "#                            if os.path.isfile(rm):\n",
    "#                                os.system('/opt/gridengine/bin/linux-x64/qsub ' \\\n",
    "#                                      + '-q ' + q + '.q ' \\\n",
    "#                                      + '-N r' + str(k) + '_' + u_id + ' ' \\\n",
    "#                                      + holdj \\\n",
    "#                                      + ' -b y -wd ' \\\n",
    "#                                      + log_dir \\\n",
    "#                                      + ' -j y -R y ' \\\n",
    "#                                      + '-pe smp ' + rm_cores\n",
    "#                                      + ' -V ' + rm_script + ' ' \\\n",
    "#                                      + rm + ' ' + u_id + ' ' + script_dir\n",
    "#                                    )\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
